# Transformer-architecture-presentation
üí° An in-depth exploration of Transformer architecture, starting from Sequence-to-Sequence (Seq2Seq) models and progressing to advanced attention mechanisms. based on ‚ÄúAttention Is All You Need‚Äù. 

**Author:** Bahar Niknam  
**Format:** PDF Slide Deck  

---

## Overview

This repository contains a concise slide deck introducing the core concepts behind **Transformer architecture**. 

### Topics Covered

- Introduction to Seq2Seq Models  
- Key Components of a Seq2Seq Model  
- RNN-Based Architectures as a Seq2Seq Model
- Attention Mechanisms:
  - Queries, Keys and Values  
  - Scaled Dot-Product Attention  
  - Additive Attention  
  - Bahdanau Attention (for RNN-based model)  
  - Multi-Head Attention  
  - Self-Attention & Positional Encoding
- Comparing CNNs, RNNs, and Self-Attention in Seq2Seq Model 
- The Transformer Architecture (Encoder & Decoder)

---

## References

- Vaswani et al. ‚Äì *Attention Is All You Need* (2017)  
- Ian Goodfellow et al. ‚Äì *Deep Learning*  
- Fran√ßois Chollet ‚Äì *Deep Learning with Python*  
- Denis Rothman ‚Äì *Transformers for NLP*  

---

## File Included

- `Transformers.pdf` ‚Äì Full slide presentation
