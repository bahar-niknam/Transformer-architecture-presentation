# Transformer-architecture-presentation
💡 An in-depth exploration of Transformer architecture, starting from Seq2Seq models and progressing to advanced attention mechanisms. based on “Attention Is All You Need”. 
