# Transformer-architecture-presentation
üí° An in-depth exploration of Transformer architecture, starting from Seq2Seq models and progressing to advanced attention mechanisms. based on ‚ÄúAttention Is All You Need‚Äù. 

**Author:** Bahar Niknam  
**Format:** PDF Slide Deck  

---

## Overview

This repository contains a concise slide deck introducing the core concepts behind **Transformer architecture**. 

### Topics Covered

- Introduction to Sequence-to-Sequence Models  
- Key Components of a Seq2Seq Model  
- RNN-Based Architectures  
- Attention Mechanisms:
  - Queries, Keys, and Values  
  - Scaled Dot-Product Attention  
  - Additive Attention  
  - Bahdanau Attention (RNN-based)  
  - Multi-Head Attention  
  - Self-Attention & Positional Encoding  
- The Transformer Architecture (Encoder & Decoder)

---

## References

- Vaswani et al. ‚Äì *Attention Is All You Need* (2017)  
- Ian Goodfellow et al. ‚Äì *Deep Learning*  
- Fran√ßois Chollet ‚Äì *Deep Learning with Python*  
- Denis Rothman ‚Äì *Transformers for NLP*  

---

## File Included

- `Transformers.pdf` ‚Äì Full slide presentation
