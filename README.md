# Transformer-architecture-presentation
üí° An in-depth exploration of Transformer architecture, starting from Sequence-to-Sequence (Seq2Seq) models and progressing to advanced attention mechanisms. based on ‚ÄúAttention Is All You Need‚Äù. 

**Author:** Bahar Niknam  
**Format:** PDF Slide Deck  

---

## Overview

This repository contains a concise slide deck introducing the core concepts behind **Transformer architecture**. 

### Topics Covered

- Introduction to Seq2Seq Models  
  - Applications of Seq2Seq Models
  - Working with Sequences
  - language models
  - Key Components of a Seq2Seq Model
    - Encoder 
    - Decoder
  - RNN based model as Seq2Seq Models
- Attention Mechanisms:
  - Queries, Keys and Values  
  - Scaled Dot-Product Attention  
  - Additive Attention  
  - Bahdanau Attention (for RNN-based model)  
  - Multi-Head Attention  
  - Self-Attention & Positional Encoding
- Comparing CNNs, RNNs, and Self-Attention in Seq2Seq Model
- Residual Connections used in transformer Architecture
- The Transformer Architecture
  - The Encoder stack
  - The Decoder stack
---

## References

- Vaswani et al. ‚Äì *Attention Is All You Need* (2017)  
- Ian Goodfellow et al. ‚Äì *Deep Learning*  
- Fran√ßois Chollet ‚Äì *Deep Learning with Python*  
- Denis Rothman ‚Äì *Transformers for NLP*  

---

## File Included

- `Transformers.pdf` ‚Äì Full slide presentation
